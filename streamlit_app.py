sp1 = '''
æ¸‹æ²¢æ „ä¸€ã®æ¥­ç¸¾ã«ã¤ã„ã¦ç†è§£ã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹å°ãƒ†ã‚¹ãƒˆã‚’ï¼”å•ï¼ˆï¼•æŠå•é¡Œã‚’ï¼’å•ã€ç©´åŸ‹ã‚å•é¡Œã‚’ï¼’å•ï¼‰ä½œæˆã—ã¦ã€‚
ä½œæˆã—ãŸï¼”ã¤ã®å•é¡Œã‚’ã€ä¸€åº¦ã«ã¯ï¼‘å•ã ã‘ã€é †ç•ªã«å‡ºé¡Œã—ã€ãã‚Œã«å¯¾ã™ã‚‹ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®ç­”ãˆã«å¯¾ã—ã¦ã€æ­£è§£ã‹ä¸æ­£è§£ã‹ã‚’ç­”ãˆã¦ã€ãã®è§£èª¬ã‚‚ç¤ºã—ã¦ã€‚
ãã‚Œã‚’ï¼”å•ã«ã¤ã„ã¦ç¹°ã‚Šè¿”ã—ã¦ã€æœ€å¾Œã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒç†è§£ã—ã¦ã„ãªã„éƒ¨åˆ†ãŒã©ã®éƒ¨åˆ†ã‹ã‚’ç¤ºã—ã¦ã€‚
'''
import streamlit as st
import google.generativeai as genai

# ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’è¡¨ç¤º
st.title("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.write(
    "ã“ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¯ Google Gemini Flash-2.5 ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚"
)

# Gemini API ã‚­ãƒ¼ã‚’ Streamlit ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‹ã‚‰å–å¾—
api_key = st.secrets.get("gemini_api_key", "")

if not api_key:
    st.info("Gemini API ã‚­ãƒ¼ã‚’ .streamlit/secrets.toml ã® 'gemini_api_key' ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚", icon="ğŸ—ï¸")
else:
    # Google Generative AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è¨­å®š
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash")

    # ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿å­˜ç”¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆå¤‰æ•°
    if "messages" not in st.session_state:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æœ€åˆã«è¿½åŠ ï¼ˆrole: userã«ã™ã‚‹ï¼‰
        st.session_state.messages = [
            {
                "role": "user",
                "content": "æ¸‹æ²¢æ „ä¸€ã«ã¤ã„ã¦ã®ã‚¯ã‚¤ã‚ºã‚’ï¼’å•ã€å‡ºé¡Œã—ã¦"
            }
        ]
        # ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«ã‚¯ã‚¤ã‚ºå‡ºé¡Œï¼ˆGemini APIå‘¼ã³å‡ºã—ï¼‰
        history = [{"role": "user", "parts": ["æ¸‹æ²¢æ „ä¸€ã«ã¤ã„ã¦ã®ã‚¯ã‚¤ã‚ºã‚’ï¼’å•ã€å‡ºé¡Œã—ã¦"]}]
        try:
            response = model.generate_content(history)
            content = response.candidates[0].content.parts[0].text
            st.session_state.messages.append({"role": "assistant", "content": content})
        except Exception as e:
            st.session_state.messages.append({"role": "assistant", "content": f"ã‚¨ãƒ©ãƒ¼: {e}"})

    # æ—¢å­˜ã®ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    for message in st.session_state.messages:
        if message["role"] == "system":
            continue
        with st.chat_message("user" if message["role"] == "user" else "assistant"):
            st.markdown(message["content"])

    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›æ¬„ï¼ˆæ—¥æœ¬èªè¡¨ç¤ºï¼‰
    if prompt := st.chat_input("è³ªå•ã‚„å›ç­”ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Gemini API ç”¨ã«å±¥æ­´ã‚’æ•´å½¢
        history = []
        for m in st.session_state.messages:
            if m["role"] == "user":
                history.append({"role": "user", "parts": [m["content"]]})
            elif m["role"] == "assistant":
                history.append({"role": "model", "parts": [m["content"]]})
            # "system" ãƒ­ãƒ¼ãƒ«ã¯å«ã‚ãªã„

        # Gemini API ã§å›ç­”ç”Ÿæˆ
        try:
            response_stream = model.generate_content(
                history,
                stream=True,
            )
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ã§å›ç­”ã‚’å…¨æ–‡ã¾ã¨ã‚ã¦è¡¨ç¤º
            with st.chat_message("assistant"):
                full_response = ""
                for chunk in response_stream:
                    if chunk.candidates and chunk.candidates[0].content.parts:
                        part = chunk.candidates[0].content.parts[0].text
                        full_response += part
                st.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
